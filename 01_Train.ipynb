{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import rastervision\n",
    "import rasterio\n",
    "\n",
    "# os.environ['GDAL_DATA'] = check_output('pip show rasterio | grep Location | awk \\'{print $NF\"/rasterio/gdal_data/\"}\\'', shell=True).decode().strip()\n",
    "# os.environ['AWS_NO_SIGN_REQUEST'] = 'YES'\n",
    "\n",
    "\n",
    "# Find the location of the 'rasterio' package using pip\n",
    "try:\n",
    "    result = subprocess.check_output('pip show rasterio', shell=True).decode('utf-8')\n",
    "\n",
    "    # Extract the location of the 'rasterio' package\n",
    "    for line in result.splitlines():\n",
    "        if line.startswith('Location:'):\n",
    "            rasterio_location = line.split(':')[-1].strip()\n",
    "            break\n",
    "    \n",
    "    # Set GDAL_DATA environment variable\n",
    "    gdal_data_path = os.path.join(rasterio_location, 'rasterio', 'gdal_data')\n",
    "    os.environ['GDAL_DATA'] = gdal_data_path\n",
    "    print(f\"GDAL_DATA has been set to: {gdal_data_path}\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error finding the location of rasterio:\", e)\n",
    "\n",
    "# Set AWS_NO_SIGN_REQUEST environment variable\n",
    "os.environ['AWS_NO_SIGN_REQUEST'] = 'YES'\n",
    "print(\"AWS_NO_SIGN_REQUEST has been set to: YES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import glob\n",
    "\n",
    "from rastervision.core.data import RasterioSource, MinMaxTransformer\n",
    "\n",
    "from rastervision.core.data import (\n",
    "    ClassConfig, GeoJSONVectorSource, RasterioCRSTransformer,\n",
    "    RasterizedSource, ClassInferenceTransformer)\n",
    "\n",
    "from rastervision.core.data import SemanticSegmentationLabelSource\n",
    "\n",
    "from rastervision.core.data.utils.geojson import get_polygons_from_uris\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from rastervision.pytorch_learner import (\n",
    "    SemanticSegmentationRandomWindowGeoDataset, SemanticSegmentationSlidingWindowGeoDataset, SemanticSegmentationVisualizer)\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from rastervision.pytorch_learner import SemanticSegmentationGeoDataConfig\n",
    "from rastervision.pytorch_learner import SolverConfig\n",
    "from rastervision.pytorch_learner import SemanticSegmentationLearnerConfig\n",
    "from rastervision.pytorch_learner import SemanticSegmentationLearner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'raster-vision/rastervision_core': Expected end or semicolon (after name and no valid version specifier)\n",
      "    raster-vision/rastervision_core\n",
      "                 ^\n"
     ]
    }
   ],
   "source": [
    "!pip install raster-vision/rastervision_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_val\n",
    "annotations_val\n",
    "images_train\n",
    "annotations_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí definimos las clases (tipos de objeto) presentes en nuestras anotaciones\n",
    "# en este caso tenemos sólo una, que indicamos como \"basural\" .\n",
    "# hay un clase adicional, implícita, que es \"background\" -el fondo, todo lo que\n",
    "# no corresponde a objetos de intéres\n",
    "\n",
    "class_config = ClassConfig(\n",
    "    names=['background', 'vivienda precaria'],\n",
    "    colors=['lightgray', 'darkred'],\n",
    "    null_class='background')\n",
    "\n",
    "\n",
    "# el tamaño en píxeles de los recortes cuadrados\n",
    "window_size = 480\n",
    "\n",
    "# Aquí definimos algunas transformaciones a realizar a los recorte del dataset\n",
    "# de entrenamiento: cambiar al azar la saturación, el brillo, rotarlos, ocultar\n",
    "# algunos pixeles. Todo esto sirve para entrenar un algoritmo de detección\n",
    "# más robusto a diferencias que puedan tener las futuras imágenes a las que\n",
    "# se aplique\n",
    "data_augmentation_transform = A.Compose([\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(),\n",
    "    A.OneOf([\n",
    "        A.HueSaturationValue(hue_shift_limit=10),\n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.RandomGamma(),\n",
    "    ]),\n",
    "    A.CoarseDropout(max_height=int(window_size/6), max_width=int(window_size/6), max_holes=4)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n",
    "    class_config=class_config,\n",
    "    aoi_uri=AoI_file_dest,\n",
    "    # aoi_uri=annotations_file_dest,\n",
    "    image_uri=images_val,\n",
    "    label_vector_uri=annotations_val,\n",
    "    label_vector_default_class_id=class_config.get_class_id('vivienda precaria'),\n",
    "    image_raster_source_kw=dict(allow_streaming=True, raster_transformers=[MinMaxTransformer()]),\n",
    "    size=window_size,\n",
    "    stride=window_size,\n",
    "    transform=A.Resize(window_size, window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = SemanticSegmentationVisualizer(\n",
    "    class_names=class_config.names, class_colors=class_config.colors)\n",
    "\n",
    "x, y = val_ds[1]\n",
    "\n",
    "vis.plot_batch(x.unsqueeze(0), y.unsqueeze(0), show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí definimos cuantos recortes sobre la imagen tomaremos al azar\n",
    "# idealmente, varias veces la cantidad de recortes en el dataset de validación\n",
    "# (algo así como sample_size = len(val_ds) * 5\n",
    "# pero como estos datos son \"pesados\" vamos a limitarnos a tomar una muestra de la misma cantidad\n",
    "sample_size = len(val_ds)*5\n",
    "\n",
    "train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(\n",
    "    class_config=class_config,\n",
    "    # aoi_uri=AoI_file_dest,\n",
    "    image_uri=images_train,\n",
    "    label_vector_uri=annotations_train,\n",
    "    label_vector_default_class_id=class_config.get_class_id('asentamiento informal'),\n",
    "    image_raster_source_kw=dict(allow_streaming=True, raster_transformers=[MinMaxTransformer()]),\n",
    "    # window sizes will randomly vary from 100x100 to 300x300\n",
    "    #size_lims=(100, 300),\n",
    "    # fixed window size\n",
    "    size_lims=(window_size, window_size+1),\n",
    "    # resize chips before returning\n",
    "    out_size=window_size,\n",
    "    # allow windows to overflow the extent by 100 pixels\n",
    "    padding=100,\n",
    "    max_windows=sample_size, # pero como estos datos son \"pesados\" vamos a limitarnos a tomar una muestra de la misma cantidad\n",
    "    transform=data_augmentation_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = vis.get_batch(train_ds, 8)\n",
    "\n",
    "vis.plot_batch(x, y, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load(\n",
    "    'AdeelH/pytorch-fpn:0.3',\n",
    "    'make_fpn_resnet',\n",
    "    name='resnet18',\n",
    "    fpn_type='panoptic',\n",
    "    num_classes=len(class_config),\n",
    "    fpn_channels=128,\n",
    "    in_channels=3,\n",
    "    out_size=(window_size, window_size),\n",
    "    pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.train(epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_model_bundle()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
